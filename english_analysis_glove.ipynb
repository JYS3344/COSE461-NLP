{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "english_analysis_glove_dual.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkoAkP79Q6mN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import statistics \n",
        "from statistics import mode\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import PorterStemmer \n",
        "import sklearn.metrics as metrics\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoARzsQLoHdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4150a199-f53b-4454-b9d7-6e81f5adbd45"
      },
      "source": [
        "#Load data from json.\n",
        "#Data has string and target has category.\n",
        "train_data_data_original_file = []\n",
        "train_data_target_file = []\n",
        "test_data_data_original_file = []\n",
        "test_data_target_file = []\n",
        "\n",
        "###########################################################\n",
        "#                                                         #\n",
        "# CHANGE path_dir to EmotionLines directory               #\n",
        "# It should end with '/'                                  #\n",
        "###########################################################\n",
        "path_dir = 'drive/My Drive/ColabNotebooks/English/EmotionLines/'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dict_emotion_value = { 'joy':0, 'neutral' : 1,'anger':2, 'disgust':3, 'sadness':4, 'surprise':5, 'fear':6, 'non-neutral':7}\n",
        "with open(path_dir+'Friends/friends_train.json') as train_file:\n",
        "    train_data = json.load(train_file)\n",
        "    for episode in train_data:\n",
        "        for phrase in episode:\n",
        "            train_data_data_original_file.append(phrase['utterance'])\n",
        "            train_data_target_file.append(dict_emotion_value[phrase['emotion']])\n",
        "with open(path_dir + 'Friends/friends_test.json') as test_file:\n",
        "    test_data = json.load(test_file)\n",
        "    for episode in test_data:\n",
        "        for phrase in episode:\n",
        "            test_data_data_original_file.append(phrase['utterance'])\n",
        "            test_data_target_file.append(dict_emotion_value[phrase['emotion']])\n",
        "print(len(train_data_data_original_file))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLayUb6Ix2RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "all = pd.DataFrame(list(zip(train_data_data_original_file, train_data_target_file)), columns =['sentence', 'value']) \n",
        "\n",
        "train_data_data_original_all = all['sentence']\n",
        "train_data_target_all = all['value']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEbQpYF0oO54",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "cfd3c76f-7466-42ef-d09d-c64fca019997"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iygtHv5oP1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(words):\n",
        "    documents = []\n",
        "    wnl = WordNetLemmatizer()\n",
        "    for sen in range(0, len(words)):\n",
        "        # Remove all the special characters\n",
        "        document = re.sub(r'\\W', ' ', str(words[sen]))\n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "        # Lemmatization\n",
        "        document = document.split()\n",
        "        document = [wnl.lemmatize(word) for word in document]\n",
        "        result = []\n",
        "        for word in document: \n",
        "            if word not in stop_words:\n",
        "                result.append(word)\n",
        "        document = result\n",
        "        document = ' '.join(document)\n",
        "        documents.append(document)\n",
        "    return documents\n",
        "\n",
        "train_data_data_all = preprocess(train_data_data_original_all)\n",
        "test_data_data = preprocess(test_data_data_original_file)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qzf82INRDM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wordsToPadd(words):\n",
        "  # define documents\n",
        "  docs = words\n",
        "  # prepare tokenizer\n",
        "  t = Tokenizer()\n",
        "  t.fit_on_texts(docs)\n",
        "  vocab_size = len(t.word_index) + 1\n",
        "  # integer encode the documents\n",
        "  encoded_docs = t.texts_to_sequences(docs)\n",
        "  # pad documents to a max length of 10 words\n",
        "  max_length = 10\n",
        "  padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "  return padded_docs,vocab_size,t\n",
        "# define class labels\n",
        "all_labels = array(train_data_target_all)\n",
        "\n",
        "all_docs,all_vocab,all_t = wordsToPadd(train_data_data_all)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ryNjjy4REM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "ac89add9-7377-4921-ad7a-6f7c6f51c442"
      },
      "source": [
        "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -n glove*.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-29 06:32:49--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-06-29 06:32:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-06-29 06:32:50--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.87MB/s    in 6m 30s  \n",
            "\n",
            "2020-06-29 06:39:20 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL9EPsEKTvWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac5cf5e5-c2e2-446c-a319-c26de82a76bd"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('./glove.6B.300d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHG-6tpmRIA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((all_vocab, 300))\n",
        "for word, i in all_t.word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_1wkgpjuHD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define documents\n",
        "test_encoded_docs_all = all_t.texts_to_sequences(test_data_data)\n",
        "max_length = 10\n",
        "test_padded_docs_all = pad_sequences(test_encoded_docs_all, maxlen=max_length, padding='post')\n",
        "test_labels = array(test_data_target_file)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZQsrlF0L_Es",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "6521c484-e2fd-4da5-f315-1cecbfdf0f42"
      },
      "source": [
        "from keras.layers import Reshape, Conv2D,Conv1D, GlobalMaxPooling2D,LSTM,MaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adagrad\n",
        "def imdb_cnn_2():\n",
        "    sequence_length=10\n",
        "    embedding_dimension=100\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(all_vocab, 300, weights=[embedding_matrix], input_length=10, trainable=False))\n",
        "    model.add(Conv1D(512,3,padding='same',activation='relu',strides=1))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv1D(64,3,padding='same',activation='relu',strides=1))\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128,activation='relu'))\n",
        "    model.add(Dense(32,activation='softmax'))\n",
        "    \n",
        "    opt = Adagrad(lr = 0.001)\n",
        "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "model = imdb_cnn_2()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 10, 300)           1467900   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 10, 512)           461312    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 5, 64)             98368     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 1, 64)             0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                4128      \n",
            "=================================================================\n",
            "Total params: 2,040,028\n",
            "Trainable params: 572,128\n",
            "Non-trainable params: 1,467,900\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJAlwXMwwUcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1fed8c0e-13e3-4525-c612-ccb9872ebd8d"
      },
      "source": [
        "model.fit(all_docs, all_labels, epochs=50,validation_split=0.2,verbose=2)#, verbose=0)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8448 samples, validate on 2113 samples\n",
            "Epoch 1/50\n",
            " - 2s - loss: 1.8321 - sparse_categorical_accuracy: 0.4498 - val_loss: 1.6666 - val_sparse_categorical_accuracy: 0.4520\n",
            "Epoch 2/50\n",
            " - 1s - loss: 1.6319 - sparse_categorical_accuracy: 0.4616 - val_loss: 1.6197 - val_sparse_categorical_accuracy: 0.4558\n",
            "Epoch 3/50\n",
            " - 1s - loss: 1.5818 - sparse_categorical_accuracy: 0.4691 - val_loss: 1.5924 - val_sparse_categorical_accuracy: 0.4581\n",
            "Epoch 4/50\n",
            " - 1s - loss: 1.5537 - sparse_categorical_accuracy: 0.4718 - val_loss: 1.5745 - val_sparse_categorical_accuracy: 0.4600\n",
            "Epoch 5/50\n",
            " - 1s - loss: 1.5281 - sparse_categorical_accuracy: 0.4786 - val_loss: 1.5568 - val_sparse_categorical_accuracy: 0.4643\n",
            "Epoch 6/50\n",
            " - 1s - loss: 1.5112 - sparse_categorical_accuracy: 0.4818 - val_loss: 1.5474 - val_sparse_categorical_accuracy: 0.4699\n",
            "Epoch 7/50\n",
            " - 1s - loss: 1.4855 - sparse_categorical_accuracy: 0.4899 - val_loss: 1.5367 - val_sparse_categorical_accuracy: 0.4709\n",
            "Epoch 8/50\n",
            " - 1s - loss: 1.4677 - sparse_categorical_accuracy: 0.4934 - val_loss: 1.5255 - val_sparse_categorical_accuracy: 0.4723\n",
            "Epoch 9/50\n",
            " - 1s - loss: 1.4557 - sparse_categorical_accuracy: 0.4968 - val_loss: 1.5175 - val_sparse_categorical_accuracy: 0.4728\n",
            "Epoch 10/50\n",
            " - 1s - loss: 1.4437 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.5096 - val_sparse_categorical_accuracy: 0.4742\n",
            "Epoch 11/50\n",
            " - 1s - loss: 1.4371 - sparse_categorical_accuracy: 0.5028 - val_loss: 1.5019 - val_sparse_categorical_accuracy: 0.4766\n",
            "Epoch 12/50\n",
            " - 1s - loss: 1.4228 - sparse_categorical_accuracy: 0.5070 - val_loss: 1.4991 - val_sparse_categorical_accuracy: 0.4766\n",
            "Epoch 13/50\n",
            " - 1s - loss: 1.4162 - sparse_categorical_accuracy: 0.5059 - val_loss: 1.4956 - val_sparse_categorical_accuracy: 0.4789\n",
            "Epoch 14/50\n",
            " - 1s - loss: 1.4042 - sparse_categorical_accuracy: 0.5078 - val_loss: 1.4901 - val_sparse_categorical_accuracy: 0.4789\n",
            "Epoch 15/50\n",
            " - 1s - loss: 1.3987 - sparse_categorical_accuracy: 0.5110 - val_loss: 1.4855 - val_sparse_categorical_accuracy: 0.4794\n",
            "Epoch 16/50\n",
            " - 1s - loss: 1.3910 - sparse_categorical_accuracy: 0.5111 - val_loss: 1.4851 - val_sparse_categorical_accuracy: 0.4827\n",
            "Epoch 17/50\n",
            " - 1s - loss: 1.3836 - sparse_categorical_accuracy: 0.5140 - val_loss: 1.4858 - val_sparse_categorical_accuracy: 0.4898\n",
            "Epoch 18/50\n",
            " - 1s - loss: 1.3732 - sparse_categorical_accuracy: 0.5159 - val_loss: 1.4769 - val_sparse_categorical_accuracy: 0.4799\n",
            "Epoch 19/50\n",
            " - 1s - loss: 1.3685 - sparse_categorical_accuracy: 0.5187 - val_loss: 1.4795 - val_sparse_categorical_accuracy: 0.4865\n",
            "Epoch 20/50\n",
            " - 1s - loss: 1.3615 - sparse_categorical_accuracy: 0.5186 - val_loss: 1.4788 - val_sparse_categorical_accuracy: 0.4889\n",
            "Epoch 21/50\n",
            " - 1s - loss: 1.3546 - sparse_categorical_accuracy: 0.5207 - val_loss: 1.4718 - val_sparse_categorical_accuracy: 0.4841\n",
            "Epoch 22/50\n",
            " - 1s - loss: 1.3497 - sparse_categorical_accuracy: 0.5258 - val_loss: 1.4693 - val_sparse_categorical_accuracy: 0.4856\n",
            "Epoch 23/50\n",
            " - 1s - loss: 1.3393 - sparse_categorical_accuracy: 0.5266 - val_loss: 1.4755 - val_sparse_categorical_accuracy: 0.4860\n",
            "Epoch 24/50\n",
            " - 1s - loss: 1.3346 - sparse_categorical_accuracy: 0.5260 - val_loss: 1.4711 - val_sparse_categorical_accuracy: 0.4818\n",
            "Epoch 25/50\n",
            " - 1s - loss: 1.3325 - sparse_categorical_accuracy: 0.5309 - val_loss: 1.4700 - val_sparse_categorical_accuracy: 0.4860\n",
            "Epoch 26/50\n",
            " - 1s - loss: 1.3236 - sparse_categorical_accuracy: 0.5324 - val_loss: 1.4650 - val_sparse_categorical_accuracy: 0.4856\n",
            "Epoch 27/50\n",
            " - 1s - loss: 1.3207 - sparse_categorical_accuracy: 0.5298 - val_loss: 1.4637 - val_sparse_categorical_accuracy: 0.4804\n",
            "Epoch 28/50\n",
            " - 1s - loss: 1.3175 - sparse_categorical_accuracy: 0.5323 - val_loss: 1.4710 - val_sparse_categorical_accuracy: 0.4860\n",
            "Epoch 29/50\n",
            " - 1s - loss: 1.3076 - sparse_categorical_accuracy: 0.5366 - val_loss: 1.4632 - val_sparse_categorical_accuracy: 0.4860\n",
            "Epoch 30/50\n",
            " - 1s - loss: 1.3027 - sparse_categorical_accuracy: 0.5405 - val_loss: 1.4638 - val_sparse_categorical_accuracy: 0.4889\n",
            "Epoch 31/50\n",
            " - 1s - loss: 1.3060 - sparse_categorical_accuracy: 0.5378 - val_loss: 1.4638 - val_sparse_categorical_accuracy: 0.4889\n",
            "Epoch 32/50\n",
            " - 1s - loss: 1.2924 - sparse_categorical_accuracy: 0.5388 - val_loss: 1.4597 - val_sparse_categorical_accuracy: 0.4851\n",
            "Epoch 33/50\n",
            " - 1s - loss: 1.2959 - sparse_categorical_accuracy: 0.5385 - val_loss: 1.4592 - val_sparse_categorical_accuracy: 0.4841\n",
            "Epoch 34/50\n",
            " - 1s - loss: 1.2939 - sparse_categorical_accuracy: 0.5434 - val_loss: 1.4623 - val_sparse_categorical_accuracy: 0.4832\n",
            "Epoch 35/50\n",
            " - 1s - loss: 1.2865 - sparse_categorical_accuracy: 0.5406 - val_loss: 1.4659 - val_sparse_categorical_accuracy: 0.4870\n",
            "Epoch 36/50\n",
            " - 1s - loss: 1.2794 - sparse_categorical_accuracy: 0.5440 - val_loss: 1.4600 - val_sparse_categorical_accuracy: 0.4856\n",
            "Epoch 37/50\n",
            " - 1s - loss: 1.2811 - sparse_categorical_accuracy: 0.5475 - val_loss: 1.4610 - val_sparse_categorical_accuracy: 0.4846\n",
            "Epoch 38/50\n",
            " - 1s - loss: 1.2738 - sparse_categorical_accuracy: 0.5464 - val_loss: 1.4562 - val_sparse_categorical_accuracy: 0.4832\n",
            "Epoch 39/50\n",
            " - 1s - loss: 1.2661 - sparse_categorical_accuracy: 0.5521 - val_loss: 1.4585 - val_sparse_categorical_accuracy: 0.4832\n",
            "Epoch 40/50\n",
            " - 1s - loss: 1.2620 - sparse_categorical_accuracy: 0.5534 - val_loss: 1.4563 - val_sparse_categorical_accuracy: 0.4823\n",
            "Epoch 41/50\n",
            " - 1s - loss: 1.2533 - sparse_categorical_accuracy: 0.5545 - val_loss: 1.4600 - val_sparse_categorical_accuracy: 0.4851\n",
            "Epoch 42/50\n",
            " - 1s - loss: 1.2558 - sparse_categorical_accuracy: 0.5508 - val_loss: 1.4529 - val_sparse_categorical_accuracy: 0.4775\n",
            "Epoch 43/50\n",
            " - 1s - loss: 1.2522 - sparse_categorical_accuracy: 0.5545 - val_loss: 1.4647 - val_sparse_categorical_accuracy: 0.4865\n",
            "Epoch 44/50\n",
            " - 1s - loss: 1.2466 - sparse_categorical_accuracy: 0.5619 - val_loss: 1.4605 - val_sparse_categorical_accuracy: 0.4813\n",
            "Epoch 45/50\n",
            " - 1s - loss: 1.2428 - sparse_categorical_accuracy: 0.5565 - val_loss: 1.4608 - val_sparse_categorical_accuracy: 0.4841\n",
            "Epoch 46/50\n",
            " - 1s - loss: 1.2316 - sparse_categorical_accuracy: 0.5608 - val_loss: 1.4574 - val_sparse_categorical_accuracy: 0.4799\n",
            "Epoch 47/50\n",
            " - 1s - loss: 1.2301 - sparse_categorical_accuracy: 0.5644 - val_loss: 1.4594 - val_sparse_categorical_accuracy: 0.4813\n",
            "Epoch 48/50\n",
            " - 1s - loss: 1.2381 - sparse_categorical_accuracy: 0.5632 - val_loss: 1.4591 - val_sparse_categorical_accuracy: 0.4813\n",
            "Epoch 49/50\n",
            " - 1s - loss: 1.2318 - sparse_categorical_accuracy: 0.5593 - val_loss: 1.4571 - val_sparse_categorical_accuracy: 0.4808\n",
            "Epoch 50/50\n",
            " - 1s - loss: 1.2243 - sparse_categorical_accuracy: 0.5626 - val_loss: 1.4603 - val_sparse_categorical_accuracy: 0.4808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9c000856a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjCuQkbltjpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d418b00a-28ad-42d4-c6fd-876012f72e03"
      },
      "source": [
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(test_padded_docs_all, test_labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 48.191029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0TZr2LxOb8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "a20d50c0-7855-433b-aefa-687f0f0fd276"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "prediction = model.predict(test_padded_docs_all)\n",
        "predicted_classes = np.argmax(prediction, axis=1)\n",
        "target_names = ['joy', 'neutral','anger', 'disgust', 'sadness', 'surprise', 'fear', 'non-neutral']\n",
        "print(prediction)\n",
        "print(classification_report(predicted_classes,test_data_target_file,zero_division=0,target_names=target_names))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.7903012e-02 5.6206602e-01 1.2819107e-02 ... 5.4554130e-09\n",
            "  1.1195650e-10 4.0160131e-10]\n",
            " [1.1761870e-01 2.5012287e-01 2.9516708e-02 ... 1.0930726e-08\n",
            "  2.1185843e-10 6.4538314e-10]\n",
            " [2.2993119e-02 7.7922696e-01 1.3228792e-02 ... 2.8364326e-07\n",
            "  1.2011922e-08 4.0843517e-08]\n",
            " ...\n",
            " [2.9012486e-01 5.3786576e-01 1.7893644e-03 ... 3.4620984e-10\n",
            "  3.9646203e-12 1.0348255e-11]\n",
            " [1.5637996e-02 2.2685820e-01 1.3083334e-01 ... 3.8912140e-07\n",
            "  1.3828943e-08 3.7705899e-08]\n",
            " [5.4549344e-02 4.7969446e-01 8.1696436e-02 ... 1.6966677e-06\n",
            "  9.6964385e-08 2.6514314e-07]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.24      0.42      0.30       171\n",
            "     neutral       0.81      0.55      0.65      1891\n",
            "       anger       0.06      0.28      0.10        36\n",
            "     disgust       0.09      0.40      0.14        15\n",
            "     sadness       0.19      0.37      0.25        43\n",
            "    surprise       0.30      0.41      0.34       209\n",
            "        fear       0.03      0.14      0.05         7\n",
            " non-neutral       0.19      0.27      0.22       392\n",
            "\n",
            "    accuracy                           0.48      2764\n",
            "   macro avg       0.24      0.35      0.26      2764\n",
            "weighted avg       0.62      0.48      0.53      2764\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}